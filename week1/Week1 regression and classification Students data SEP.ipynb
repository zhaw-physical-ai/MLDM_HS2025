{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dedc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04dce1",
   "metadata": {},
   "source": [
    "# Data generation, exploration, regression, then classification, then clustering, then RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5b5cd",
   "metadata": {},
   "source": [
    " * Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9db62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a4a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d43ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# initial dataset\n",
    "data = {\n",
    "    'Quizzes': [5.3, 5.5, 5.5, 5.0, 2.5, 3.0, 3.0, 3.1, 3.5, 3.5, 3.7, 4.0, 4.0, 4.0, 4.0, 5.7],\n",
    "    'Labs':    [5.3, 5.9, 5.29, 4.35, 5.5, 5.0, 4.0, 3.1, 4.5, 5.62, 5.0, 3.0, 4.0, 5.75, 4.44, 5.9],\n",
    "    'Final':   [4.818, 5.773, 4.464, 5.069, 4.164, 4.091, 3.727, 3.909, 3.518, 4.897, 5.209, 3.0, 3.091, 5.818, 4.069, 5.77]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Augmenting data by replicating and adding noise\n",
    "np.random.seed(42)\n",
    "augmented = []\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    for _ in range(10):  # Create 10 variations of each record\n",
    "        noise = np.random.normal(0, 0.2, 3)  # Add Gaussian noise with mean=0 and std=0.2\n",
    "        new_row = row + noise\n",
    "        # Ensure values are within boundaries (grades must be between 2 and 6)\n",
    "        new_row = new_row.clip(2, 6)\n",
    "        augmented.append(new_row)\n",
    "\n",
    "# Create augmented DataFrame\n",
    "df_aug = pd.DataFrame(augmented, columns=df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67653054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "\n",
    "# Print dataset shape and summary statistics\n",
    "print(\"Augmented Dataset Shape:\", df_aug.shape)\n",
    "print(\"\\nSummary Statistics (Final Grades):\")\n",
    "print(df_aug['Final'].describe())\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.hist(df_aug['Final']+0.0, bins=20, color='blue', alpha=0.6, edgecolor='black')\n",
    "plt.title(\" SEP Note Distribution\")\n",
    "plt.xlabel(\"SEP Note\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory data analysis\n",
    "\n",
    "plt.figure(figsize=(13, 3))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(df_aug['Labs'],df_aug['Final'],'o')\n",
    "\n",
    "plt.xlabel(\"Labs\")\n",
    "plt.ylabel(\"SEP Note\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(df_aug['Quizzes'],df_aug['Final'],'o')\n",
    "plt.xlabel(\"Quizzes\")\n",
    "plt.ylabel(\"SEP Note\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(df_aug['Quizzes'],df_aug['Labs'],'o')\n",
    "plt.xlabel(\"Quizzes\")\n",
    "plt.ylabel(\"Labs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target for regression\n",
    "X = df_aug[['Quizzes', 'Labs']]\n",
    "y = df_aug['Final']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Linear Regression model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(\"Regression Model Evaluation:\")\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# Visualize Actual vs Predicted\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual Final Grades\")\n",
    "plt.ylabel(\"Predicted Final Grades\")\n",
    "plt.title(\"Actual vs. Predicted Final Grades\")\n",
    "plt.plot([2, 6], [2, 6], color='red', linestyle='--')  # reference line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e17c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180656e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create binary target: 1 for pass, 0 for fail\n",
    "df_aug['Pass'] = (df_aug['Final'] > 4).astype(int)\n",
    "\n",
    "X_class = df_aug[['Quizzes', 'Labs']]\n",
    "y_class = df_aug['Pass']\n",
    "\n",
    "# Split data\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_class, y_class, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred_cls = classifier.predict(X_test_cls)\n",
    "print(\"\\nClassification Model Evaluation:\")\n",
    "print(confusion_matrix(y_test_cls, y_pred_cls))\n",
    "print(classification_report(y_test_cls, y_pred_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Plotting the Classification Result\n",
    "# ------------------------------\n",
    "# plot the decision boundary and the test set\n",
    "\n",
    "# Create a mesh grid for plotting decision boundaries\n",
    "x_min, x_max = X_class['Quizzes'].min() - 0.1, X_class['Quizzes'].max() + 0.1\n",
    "y_min, y_max = X_class['Labs'].min() - 0.1, X_class['Labs'].max() + 0.1\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Use the classifier to predict class labels for each point in the mesh grid\n",
    "Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot decision boundary by contour filling\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "\n",
    "# Plot the test data points on top; color by true class.\n",
    "scatter = plt.scatter(X_test_cls['Quizzes'], X_test_cls['Labs'], \n",
    "                      c=y_test_cls, edgecolor='k', cmap=plt.cm.coolwarm, s=60)\n",
    "plt.xlabel(\"Quizzes\")\n",
    "plt.ylabel(\"Labs\")\n",
    "plt.title(\"Logistic Regression Decision Boundary and Test Data\")\n",
    "# Create a legend for Pass/Fail labels.\n",
    "legend_labels = {0: 'Fail (SEP Note < 4)', 1: 'Pass (SEP Note > 4)'}\n",
    "handles = [plt.Line2D([], [], marker='o', color='w', markerfacecolor=plt.cm.coolwarm(color/1.0), markersize=10,\n",
    "                      markeredgecolor='k') for color in [0, 1]]\n",
    "plt.legend(handles, [legend_labels[i] for i in [0, 1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Use only the semester grades (Quizzes and Labs)\n",
    "X_cluster = df_aug[['Quizzes', 'Labs']]\n",
    "\n",
    "# Choose a number of clusters, e.g., 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "df_aug['Cluster'] = clusters\n",
    "\n",
    "# Visualize clustering results\n",
    "plt.scatter(df_aug['Quizzes'], df_aug['Labs'], c=df_aug['Cluster'], cmap='viridis', alpha=0.6)\n",
    "plt.xlabel(\"Quizzes\")\n",
    "plt.ylabel(\"Labs\")\n",
    "plt.title(\"K-Means Clustering of Student Performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 3: Map Clusters to SEP Grades\n",
    "# -----------------------------\n",
    "# Compute summary statistics of SEP Final grades for each cluster\n",
    "cluster_summary = df_aug.groupby('Cluster')['Final'].agg(['mean', 'median', 'min', 'max', 'count'])\n",
    "print(\"SEP Grade Statistics by Cluster:\")\n",
    "print(cluster_summary)\n",
    "df_aug['Pass'] = (df_aug['Final'] > 4).astype(int)  # or use >= if needed\n",
    "pass_rate = df_aug.groupby('Cluster')['Pass'].mean()\n",
    "# You can also compute the fraction of students passing (SEP > 4) per cluster:\n",
    "#pass_rate = df_aug.groupby('Cluster')['Final'].apply(lambda x: np.mean(x >= 4))\n",
    "pass_rate = pass_rate.rename(\"Pass_Rate\")\n",
    "print(\"\\nPass Rates by Cluster:\")\n",
    "print(pass_rate)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Visualize the Cluster Mapping to SEP Grades\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Bar plot for the mean SEP grade by cluster\n",
    "plt.bar(cluster_summary.index.astype(str), cluster_summary['mean'], \n",
    "        color=['purple', 'teal', 'yellow'], alpha=0.7)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Mean SEP Grade\")\n",
    "plt.title(\"Mean SEP Grade by Cluster\")\n",
    "plt.ylim(2, 6)\n",
    "\n",
    "# Add text for the pass rate over each bar\n",
    "for idx, row in cluster_summary.iterrows():\n",
    "    plt.text(x=idx, y=row['mean'] + 0.1,\n",
    "             s=f\"Pass Rate: {pass_rate.loc[idx]*100:.1f}%\", ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: Scatterplot with Cluster Colors and SEP Grade as a Label\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(df_aug['Quizzes'], df_aug['Labs'], \n",
    "                      c=df_aug['Cluster'], cmap='viridis', alpha=0.6, edgecolor='k')\n",
    "plt.xlabel(\"Quizzes\")\n",
    "plt.ylabel(\"Labs\")\n",
    "plt.title(\"Clusters of Students (Colored by Cluster) with SEP Grade Indication\")\n",
    "\n",
    "# Overlay mean SEP grade per cluster at the centroid of each cluster (approx.)\n",
    "centers = kmeans.cluster_centers_\n",
    "for i, center in enumerate(centers):\n",
    "    # Get corresponding SEP grades within that cluster to compute a representative value\n",
    "    grade_avg = df_aug[df_aug['Cluster'] == i]['Final'].mean()\n",
    "    plt.text(center[0], center[1], f\"{grade_avg:.2f}\",\n",
    "             fontsize=12, fontweight='bold', color='white', ha='center', va='center',\n",
    "             bbox=dict(facecolor='black', alpha=0.6))\n",
    "\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e37aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(df_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f87f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class StudentPerformanceEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom environment to simulate student performance and intervention.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(StudentPerformanceEnv, self).__init__()\n",
    "        # State: [QuizAvg, LabAvg, CurrentStep] - normalized between 0 and 1\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)\n",
    "        # Actions: 0 = no intervention, 1 = light intervention, 2 = intensive intervention\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.max_steps = 10  # simulate 10 time steps in a semester\n",
    "        self.current_step = 0\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        # Random initial performance (normalized from original 2 to 6 -> scale to 0-1)\n",
    "        quiz = np.random.uniform(2, 6)\n",
    "        lab = np.random.uniform(2, 6)\n",
    "        self.current_step = 0\n",
    "        self.state = np.array([ (quiz-2)/4, (lab-2)/4, 0.0 ])\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        # Decode current performance from state:\n",
    "        quiz_score = self.state[0] * 4 + 2\n",
    "        lab_score = self.state[1] * 4 + 2\n",
    "        \n",
    "        # Apply intervention effect\n",
    "        # For demonstration: interventions boost scores by a factor (with diminishing returns)\n",
    "        if action == 0:\n",
    "            delta = 0.0\n",
    "        elif action == 1:\n",
    "            delta = np.random.uniform(0.1, 0.3)\n",
    "        else:  # action 2: intensive intervention\n",
    "            delta = np.random.uniform(0.3, 0.6)\n",
    "        \n",
    "        # Update both quiz and lab scores (as a simple simulation)\n",
    "        quiz_score = min(6, quiz_score + delta)\n",
    "        lab_score  = min(6, lab_score + delta)\n",
    "        \n",
    "        # Update state: update the scores and current step progress\n",
    "        progress = self.current_step / self.max_steps\n",
    "        self.state = np.array([ (quiz_score-2)/4, (lab_score-2)/4, progress ])\n",
    "\n",
    "        # At the final time step, determine final exam grade as a function of performance\n",
    "        if self.current_step == self.max_steps:\n",
    "            # A simple formula: weighted average of quiz and lab scores, plus some randomness\n",
    "            final_grade = (quiz_score + lab_score)/2 + np.random.uniform(-0.2, 0.2)\n",
    "            final_grade = np.clip(final_grade, 2, 6)\n",
    "            # Reward: +1 if pass (grade>4) else 0, or use the final grade as reward\n",
    "            reward = 1 if final_grade > 4 else 0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0  # no reward until the semester ends\n",
    "            done = False\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        print(f\"Step: {self.current_step}, State: {self.state}\")\n",
    "\n",
    "# Example: running a random policy on the environment\n",
    "env = StudentPerformanceEnv()\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(env.max_steps):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Total Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9516b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Define the Environment\n",
    "# ---------------------------\n",
    "class StudentPerformanceEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom environment to simulate student performance and intervention.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(StudentPerformanceEnv, self).__init__()\n",
    "        # State: [QuizNormalized, LabNormalized, Semester Progress]\n",
    "        # Quiz and Lab scores are normalized from [2,6] to [0, 1]\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)\n",
    "        # Actions: 0 = no intervention, 1 = light intervention, 2 = intensive intervention\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.max_steps = 10  # simulate 10 time steps for a semester\n",
    "        self.current_step = 0\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        # Random initial performance (scores between 2 and 6 normalized to 0-1)\n",
    "        quiz = np.random.uniform(2, 6)\n",
    "        lab = np.random.uniform(2, 6)\n",
    "        self.current_step = 0\n",
    "        self.state = np.array([ (quiz - 2) / 4, (lab - 2) / 4, 0.0 ])\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Decode current performance from the normalized state:\n",
    "        quiz_score = self.state[0] * 4 + 2\n",
    "        lab_score  = self.state[1] * 4 + 2\n",
    "        \n",
    "        # Intervention effect. More aggressive interventions yield a larger boost.\n",
    "        if action == 0:\n",
    "            delta = 0.0\n",
    "        elif action == 1:\n",
    "            delta = np.random.uniform(0.1, 0.3)\n",
    "        else:  # action == 2; intensive intervention\n",
    "            delta = np.random.uniform(0.3, 0.6)\n",
    "        \n",
    "        # Update the scores: ensuring that they do not exceed the maximum grade of 6.\n",
    "        quiz_score = min(6, quiz_score + delta)\n",
    "        lab_score  = min(6, lab_score + delta)\n",
    "        \n",
    "        # Update the semester progress\n",
    "        progress = self.current_step / self.max_steps\n",
    "        \n",
    "        # Update state with normalized quiz and lab scores\n",
    "        self.state = np.array([ (quiz_score - 2) / 4, (lab_score - 2) / 4, progress ])\n",
    "        \n",
    "        # At the final time step, compute a final exam grade influenced by the present performance\n",
    "        if self.current_step == self.max_steps:\n",
    "            # A simple rule: final exam grade as the average of current quiz and lab scores plus noise\n",
    "            final_grade = (quiz_score + lab_score) / 2 + np.random.uniform(-0.2, 0.2)\n",
    "            final_grade = np.clip(final_grade, 2, 6)\n",
    "            # Reward: +1 if passing (final_grade > 4), otherwise 0.\n",
    "            reward = 1 if final_grade > 4 else 0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        print(f\"Step: {self.current_step}, State: {self.state}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Run a Few Episodes Using a Random Policy and Collect Data for Visualization\n",
    "# ---------------------------\n",
    "env = StudentPerformanceEnv()\n",
    "\n",
    "num_episodes = 50\n",
    "episode_rewards = []\n",
    "all_episode_states = []\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_states = [state.copy()]\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(env.max_steps):\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        episode_states.append(state.copy())\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    episode_rewards.append(total_reward)\n",
    "    all_episode_states.append(episode_states)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Visualization\n",
    "# ---------------------------\n",
    "# (a) Plot a trajectory of one episode to show how the state evolves\n",
    "episode_to_plot = 0  # for example, plot the first episode\n",
    "states = np.array(all_episode_states[episode_to_plot])\n",
    "steps = np.arange(states.shape[0])\n",
    "\n",
    "# Convert normalized quiz and lab scores back to original scale (2,6)\n",
    "quiz_scores = states[:, 0] * 4 + 2\n",
    "lab_scores  = states[:, 1] * 4 + 2\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps, quiz_scores, marker='o', label='Quiz Score')\n",
    "plt.plot(steps, lab_scores, marker='o', label='Lab Score')\n",
    "plt.xlabel(\"Time Step (Semester Progress)\")\n",
    "plt.ylabel(\"Grade\")\n",
    "plt.title(\"State Evolution in an Episode\")\n",
    "plt.ylim(2, 6)\n",
    "plt.legend()\n",
    "\n",
    "# (b) Plot the distribution of final rewards over episodes\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(episode_rewards, bins=np.arange(-0.5, 2, 1), edgecolor='black', rwidth=0.8)\n",
    "plt.xlabel(\"Final Reward (Pass=1, Fail=0)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Episode Final Rewards\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d802c",
   "metadata": {},
   "source": [
    "We simulate several episodes using (for example) a random policy (i.e. selecting random interventions) and then visualize:\n",
    "The evolution of the student performance (using normalized quiz and lab scores) over the course of a single episode.\n",
    "The distribution of final rewards (pass/fail outcomes) over many episodes.\n",
    "You can imagine that, in a real RL experiment, you would replace the random action selection with an RL algorithm (such as Q-learning, Policy Gradient, or DQN) that learns to intervene more optimally. For demonstration purposes, our code uses a random agent.\n",
    "\n",
    "\n",
    "Environment Definition:\n",
    "The custom StudentPerformanceEnv models a student’s progress over a 10-step semester. At each step, the environment updates quiz and lab scores according to an intervention action. At the final step, a final grade is computed, and the episode reward is defined as ( +1 ) if the student’s final grade exceeds 4 (pass) and 0 otherwise.\n",
    "Episode Simulation:\n",
    "We simulate 50 episodes using random actions. For every episode, we record the state (which holds the normalized quiz and lab scores as well as semester progress) at each time step, and finally, note the cumulative reward.\n",
    "Visualization:\n",
    "In subplot (a), we plot the evolution of scores (converted back from the normalized state) over the time steps for one episode. This shows how interventions (even random ones) affect the simulated performance.\n",
    "In subplot (b), we plot a histogram showing the distribution of the final rewards across episodes (pass/fail outcomes). This gives an idea of how frequently the random policy leads to a pass.\n",
    "This demonstration not only provides a visual understanding of the environment’s dynamics but also sets the stage for later implementing and visualizing a learning algorithm that improves on the random policy through reinforcement learning.\n",
    "\n",
    "\n",
    "The environment in this context is a custom OpenAI Gym simulation designed to mimic a student's performance evolution over a semester in response to different interventions. In other words, instead of using real-world data directly, we construct a simulation (the environment) that models how a student's quiz and lab scores might evolve over time when certain \"actions\" (i.e., interventions) are taken.\n",
    "Key Elements of the Environment\n",
    "State:\n",
    "The state is a numerical vector representing the current student performance and progress. Specifically, it includes:\n",
    "Quiz Score (normalized): The current quiz score scaled from the original scale (2 to 6) to a normalized 0–1 range.\n",
    "Lab Score (normalized): Similarly, the current lab score normalized.\n",
    "Semester Progress: A number between 0 and 1 indicating how far along in the semester the student is (based on the current time step).\n",
    "Action Space:\n",
    "The actions available to an agent (or policy) in this environment are interventions that can affect the student's performance:\n",
    "Action 0: No intervention.\n",
    "Action 1: A light intervention (a small boost in performance).\n",
    "Action 2: An intensive intervention (a larger boost in performance).\n",
    "Dynamics (Transition Function):\n",
    "When an action is taken:\n",
    "The environment updates the quiz and lab scores by adding a small “boost” (the magnitude of which depends on the type of intervention chosen).\n",
    "The scores are capped at the maximum allowable grade (6).\n",
    "The semester progresses by one time step.\n",
    "Episode Termination and Reward:\n",
    "The environment is set to run for a fixed number of time steps (e.g., 10 steps, representing a semester).\n",
    "At the final time step, the environment calculates a final exam grade as a function of the current quiz and lab scores (plus some randomness).\n",
    "The reward is assigned based on the final grade: for instance, if the final exam grade is greater than 4 (pass), the environment might return a reward of +1; otherwise, the reward is 0.\n",
    "Purpose:\n",
    "This environment is used to demonstrate and test reinforcement learning (RL) methods. An RL agent interacts with this environment by choosing interventions at each step, and over time, it could learn which sequence of interventions maximizes the student’s chance of passing the final exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3622a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01a15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076783d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
