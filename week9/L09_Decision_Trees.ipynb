{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DaGON1W6YEL1","tags":[]},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import io\n","\n","RANDOM_SEED = 0xdeadbeef\n","%config InlineBackend.figure_formats = ['svg']\n","\n","cmap_scatter =plt.colormaps['tab10']\n","cmap= mpl.colors.ListedColormap(['red', 'blue'])"]},{"cell_type":"markdown","metadata":{"id":"27XrIqzbnQ21"},"source":["# Task 1: Train and Fine-tune a Decision Tree (2 points)\n","1. Generate a synthetic non-linearly separable dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDKFNzaFnQ21","tags":[]},"outputs":[],"source":["from sklearn import datasets\n","X, y = datasets.make_moons(n_samples=10000, noise=0.4, random_state=RANDOM_SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwj9_xDwnQ21","tags":[]},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(7, 3))\n","for i, cn in enumerate([0,1]):\n","    ax.scatter(X[y==i][:, 0],X[y==i][:, 1],\n","                         label=i,\n","                         color=cmap(i),\n","               edgecolor=\"k\"\n","              )\n","ax.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0BReRPDnQ21","tags":[]},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(7, 3))\n","ax.scatter(X[:, 0],X[:, 1], c=y, cmap=cmap,edgecolor=\"k\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pV8-k9thnQ22"},"source":["2. Split into train- and test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tybuoa_XnQ22","tags":[]},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_SEED)"]},{"cell_type":"markdown","metadata":{"id":"LZyMEhe1nQ22"},"source":["3. Use grid search with 5 fold cross-validation (with the help of the GridSearchCV class in scikit-learn) to find good hyperparameter values for a DecisionTreeClassifier  \n","https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n","\n","4. Train the model on the full training set using these hyperparameters, and measure its accuracy on the test set. You should get roughly 85% to 87% accuracy. Note that the GridSearchCV in scikit learn by default executes the hyperparameter search using cross validation. In the end it fits the final model using the best set of hyperparameters on the full training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Ci7XVbInQ22","tags":[]},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2KBDI4Gf7BK"},"outputs":[],"source":["# Some code skeleton to get you started:\n","# Create a Decision Tree classifier\n","dt_classifier = DecisionTreeClassifier()\n","\n","# Create a GridSearchCV object with cross-validation\n","# You can adjust the number of cross-validation folds (cv) as needed\n","grid_search = GridSearchCV(...)\n","\n","# Fit the GridSearchCV object to the training data\n","grid_search.fit(...)\n","\n","# Get the best hyperparameters and the corresponding model\n","best_params = ..\n","best_model = .."]},{"cell_type":"markdown","metadata":{"id":"TZgNQiRhnQ22"},"source":["5. Visualise the learned decision boundary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_CHvXSdnQ22","tags":[]},"outputs":[],"source":["from sklearn.inspection import DecisionBoundaryDisplay\n","\n","fig, ax = plt.subplots(figsize=(7, 3))\n","DecisionBoundaryDisplay.from_estimator(\n","        best_model,\n","        X,\n","        grid_resolution=300,\n","        cmap=plt.cm.Pastel1,\n","        response_method=\"predict\",\n","        ax=ax,\n","        xlabel=\"x1\",\n","        ylabel=\"x2\",\n","    )\n","\n","# ax.scatter(X[:, 0],X[:, 1], c=y, cmap=cmap, alpha=0.2)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-Mybpve6bSLB"},"source":["scikit-learn also provides some nice helper functions to visualize Decision Trees.\n","\n","One of those functions is `sklearn.tree.export_text` which returns a string representation of the decisions.\n","Have a look at the print-out of the decision tree below and the plot of its decisions on our `x1` and `x2` above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nRnUFUsbnjH","tags":[]},"outputs":[],"source":["from sklearn.tree import export_text\n","\n","print(export_text(best_model, decimals=3))"]},{"cell_type":"markdown","metadata":{"id":"FnOwChAacYZ9"},"source":["It also provides `plot_tree` which will create a nice graph representation of the classifier - even when it is very wide in thise case:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBsGOkXZnQ23","tags":[]},"outputs":[],"source":["from sklearn import tree\n","\n","fig, ax = plt.subplots(figsize=(50, 5))\n","tree.plot_tree(best_model,\n","              feature_names=[\"x1\", \"x2\"],\n","        class_names=['0','1'],\n","        rounded=True,\n","        filled=True,\n","              ax=ax)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mqhwwIOGnQ22"},"source":["## 游닉 **HAND-IN** 游닉: Report the best accuracy you reached on the test set in Task 1 in Moodle"]},{"cell_type":"markdown","metadata":{"id":"WTx_V2wMcJNf"},"source":["# Task 2: Rotation of Decision Boundaries (3 points)\n","This exericse is again on synthetic non-linearly separable dataset. We will train a decision tree classifier and investigate the effect of rotating the input data on the resulting tree."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwFSt5QiySU-","tags":[]},"outputs":[],"source":["from sklearn.datasets import make_blobs\n","\n","X, y = make_blobs(\n","    n_samples=800,\n","    n_features=2,\n","    centers=np.array([\n","        [-1., -1.],\n","        [1., -1],\n","        [1., 1.],\n","        [-1., 1.],\n","    ]),\n","    cluster_std=0.25,\n","    random_state=RANDOM_SEED,\n",")\n","y[y == 2] = 0\n","y[y == 3] = 1\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\"autumn\", edgecolors='b')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4wAZ7d0lSKOh"},"source":["### Fitting a Decision Tree and Visualising the Decision Boundaries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vl41U1i2K4IP","tags":[]},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","clf = DecisionTreeClassifier()\n","clf.fit(X, y)"]},{"cell_type":"markdown","metadata":{"id":"RHWagCH9nQ23"},"source":["Visualise the learned decision boundaries:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sj9BE4SbnQ23","tags":[]},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(6, 5))\n","DecisionBoundaryDisplay.from_estimator(\n","        clf,\n","        X,\n","        grid_resolution=300,\n","        cmap='autumn', # cmap=plt.cm.Pastel1,\n","        alpha=0.5,\n","        response_method=\"predict\",\n","        ax=ax,\n","        xlabel=\"x1\",\n","        ylabel=\"x2\",\n","    )\n","ax.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', edgecolors='b')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"u9Uck-LgWclB"},"source":["### Split Orientation\n","\n","One quirk of many implementations of Decision Trees is that the decision boundaries are always parallel to the data axes, meaning that you will never see a diagonal line as a boundary between red and yellow in our case. They will always be horizontal or vertical.\n","\n","**TASK: Reflect on why this is the case.**\n","\n","To get more intuition you can study the next cell and its output. Change the value of the variable `angle` (in radians) to rotate our synthetic data and see the influence on the decision boundaries."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"f9lRpWrVf7BL"},"outputs":[],"source":["-np.pi / 12.334"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Piti1uB07TDg","tags":[]},"outputs":[],"source":["angle = -0.25\n","\n","rot = np.array([\n","    [np.cos(angle), -np.sin(angle)],\n","    [np.sin(angle), np.cos(angle)],\n","])\n","X_rot = X @ rot\n","\n","clf2 = DecisionTreeClassifier()\n","clf2.fit(X_rot, y)"]},{"cell_type":"markdown","metadata":{"id":"3HMUqY3yf7BL"},"source":["To determine the complexity of the decision boundary you can either plot the data and count horizontal/vertical lines, or use function `clf2.get_n_leaves()`:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"czygHTaWf7BL"},"outputs":[],"source":["print(clf2.get_n_leaves())"]},{"cell_type":"markdown","metadata":{"id":"coM84-R6f7BL"},"source":["We define a helper function to plot the decision boundaries:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"tCByF3Enf7BL"},"outputs":[],"source":["def plot_dec_boundaries(ax, clf, X, y):\n","    DecisionBoundaryDisplay.from_estimator(\n","        clf,\n","        X,\n","        grid_resolution=300,\n","        cmap='autumn',\n","        alpha=0.5,\n","        response_method=\"predict\",\n","        ax=ax,\n","        xlabel=\"x1\",\n","        ylabel=\"x2\",\n","    )\n","    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', edgecolors='b')"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"csqBfrQ_f7BL"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(6, 5))\n","plot_dec_boundaries(ax, clf2, X_rot, y)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vukHVOWkkfS6"},"source":["## 游닉 **HAND-IN** 游닉: Answer the following in **Moodle**\n","Which of the following angles (in radians) produces the highest number of leaf nodes for this data?\n","* 3.14\n","* -3.14\n","* 2.61\n","* -1.31"]},{"cell_type":"markdown","metadata":{"id":"Dwecl4YNdFLn"},"source":["# Task 3: Random Forests (5 Points)\n","\n","A Random Forest is a collection of Decision Trees. One reason to combine multiple Decision Trees into a Random Forest is to counteract the tendency of Decision Trees to overfit the data.  \n","First, we generate some synthetic data. You can see that this data set has $4$ classes that mix together a lot in the middle (around the origin)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CRj1MAguI-k6","tags":[]},"outputs":[],"source":["# we are using \"_RF\" for the variable names to not confuse/mix it with the data from the previous tasks\n","\n","X_RF, y_RF = make_blobs(\n","    n_samples=800,\n","    n_features=2,\n","    random_state=0,\n","    centers=4,\n","    cluster_std=1.,\n",")\n","\n","plt.scatter(X_RF[:, 0], X_RF[:, 1], c=y_RF, cmap='autumn', edgecolors='b')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"KNnPWEoJf7BM"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X_RF, y_RF, random_state=RANDOM_SEED)"]},{"cell_type":"markdown","metadata":{"id":"eAk-4VOfekZ7"},"source":["In the cells below, we train a single Decision Tree on a training dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTOzdGyhJejV","tags":[]},"outputs":[],"source":["clf = DecisionTreeClassifier(random_state=RANDOM_SEED)\n","clf.fit(X_train, y_train)\n","fig, ax = plt.subplots(figsize=(6, 5))\n","\n","plot_dec_boundaries(ax, clf, X_RF, y_RF)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GWW4AF7pfEL_"},"source":["You should be able to see some odd artefacts in the decision regions, which indicate overfitting.\n","\n","In the next cell we use the scikit learn Random Forest implementation.\n","\n","To train a Random Forest, we train a collection of Decision Trees. Each tree is only trained on a subset (sampled with replacement) of the full training data. This is called **bootstrapping**.\n","\n","To get a prediction from a Random Forest classifier, it computes the majority vote of all its constituent Decision Trees."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"_5RyaTrJf7BT"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kL3zix5YnQ26","tags":[]},"outputs":[],"source":["forest = RandomForestClassifier(n_estimators=1, max_samples=None, random_state=RANDOM_SEED)\n","forest.fit(X_train, y_train)\n","\n","fig, ax = plt.subplots(figsize=(6, 5))\n","\n","plot_dec_boundaries(ax, forest, X_RF, y_RF)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5OheJgGAihZ-"},"source":["**TASK 3:** Apply a cross validation grid search on the hyperparameters `n_estimators` and `max_samples`. Then train the model on the full training set using these hyperparameters, and measure its accuracy on the test set."]},{"cell_type":"markdown","metadata":{"id":"mXMVIZxzjhBR"},"source":["## 游닉 **HAND-IN** 游닉: Enter the following in **Moodle**\n","\n","* Enter the final accuracy reached in **TASK 3**"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.17"}},"nbformat":4,"nbformat_minor":0}