{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pAJRKdv9QA4C"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1m4qcrpXdTt"},"outputs":[],"source":["RANDOM_SEED = 0x0"]},{"cell_type":"markdown","metadata":{"id":"oTVOKVltsG4d"},"source":["# TASK 1: Metrics (5 Points)\n","In this task you will compute some standard quality measures like Precision, Recall and F-Score for an artificial dataset."]},{"cell_type":"markdown","metadata":{"id":"Q7fZdlU5y5Dx"},"source":["First, we generate some artificial data for a binary classification task and take a look at it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZ_TBTXQfq_Z"},"outputs":[],"source":["from sklearn.datasets import make_blobs\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","X, y = make_blobs(n_samples=500, centers=2, random_state=RANDOM_SEED, cluster_std=2)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51zAL1iNwwpb"},"outputs":[],"source":["for target_class in np.unique(y):\n","  plt.scatter(X_train[y_train==target_class, 0], X_train[y_train==target_class, 1], alpha=0.75, label=target_class)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jyZqEqjVyDoO"},"source":["As we can see, the classes are not easily distinguishable, and there is no linear separator between the two classes. Nevertheless, let's apply a Logistic Regression Model and predict `y` values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T0izCFFL0WaP"},"outputs":[],"source":["log_reg = LogisticRegression(random_state=RANDOM_SEED).fit(X_train, y_train)\n","y_test_pred = log_reg.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"mxb6fCre0Yq7"},"source":["### Create and visualise the confusion matrix that describes the results:\n","\n","1. Create confusion matrix. Use can use `sklearn.metrics.confusion_matrix` functions.\n","2. Display the confusion matrix of the Logistic Regression Model with `seaborn.heatmap`. Include the numbers of samples in each cell of the heatmap."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nCZgTYP3fZe7"},"outputs":[],"source":["import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","# TODO: finish me\n","sns.heatmap(confusion_matrix_test, annot=True)"]},{"cell_type":"markdown","metadata":{"id":"Y2-ZAlHQQOG2"},"source":["### Task 1A. Create functions that calculate Precision, Recall and F1-Score (2 points)\n","1. Implement your own functions for calculating Precision, Recall and F1-Score from the confusion matrix. Don't use any of the existing libraries for this.\n","2. Apply them on the example above for the class `0`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9X3Q8Je3B3B"},"outputs":[],"source":["def precision(class_of_interest, confusion_matrix_test):\n","  return 0  # COMPLETE ME\n","\n","def recall(class_of_interest, confusion_matrix_test):\n","  return 0  # COMPLETE ME\n","\n","def f1_score(precision_value, recall_value):\n","  return 0  # COMPLETE ME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghV4-8vB7c6b"},"outputs":[],"source":["precision_value = precision(0, confusion_matrix_test)\n","recall_value = recall(0, confusion_matrix_test)\n","f1_score_test = f1_score(precision_value, recall_value)\n","\n","print(f\"Precision: {precision_value}\")\n","print(f\"Recall: {recall_value}\")\n","print(f\"F1-Score: {f1_score_test}\")"]},{"cell_type":"markdown","source":["## ðŸ“¢ **HAND-IN** ðŸ“¢:\n","\n","What is the F1-Score that you computed?"],"metadata":{"id":"eIaA5yfsOZj7"}},{"cell_type":"markdown","metadata":{"id":"LEg0WoE59k6L"},"source":["### Check your results\n","Use the function `classification_report` of `sklearn.metrics` to compare their results to your own implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZraefM_U8wdN"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_test, y_test_pred, digits=3))"]},{"cell_type":"markdown","source":["### Task 1B. Multi-Class Dataset and One-v-Rest Classification (3 points)\n","Now let's do a similar analysis for a multi-class classification task with 4 classes. First, let's make the dataset:"],"metadata":{"id":"XqG5MvIhWa8Y"}},{"cell_type":"code","source":["X, y = make_blobs(n_samples=500, centers=4, random_state=RANDOM_SEED, cluster_std=2)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"],"metadata":{"id":"edVvlzHHWo_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instead of just using the built-in multi-classification technique of sklearn.linear_model.LogisticRegression, you should build your own multi-classifier using the one-v-rest technique explained in class. In particular, train 4 different binary classifiers, one for each class. The job of classifier i is to distinguish between examples that belong to that class i from examples that belong to any other class j != i. After training the 4 classifiers, you can classify each test example based on the highest probability output by any of the 4 classfiiers (use predict_proba and select outputs ``[:,1]``, i.e., the probability corresponding to its \"positive\" class for each example)."],"metadata":{"id":"b4AWeOmEWxCg"}},{"cell_type":"code","source":["# TODO: implement one-v-rest multi-class classifier\n","\n","# Evaluate the classifier\n","print(classification_report(y_test, yhat, digits=3))"],"metadata":{"id":"OU3ZxhdSXkId"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ“¢ **HAND-IN** ðŸ“¢\n","\n","1. Your code for one-v-rest approach\n","2. The average F1 score over all 4 classes"],"metadata":{"id":"TvUwOeeFPL7U"}},{"cell_type":"markdown","source":["Again, you can compare the accuracy of the one-v-rest approach you implemented to the built-in multi-class classification approach of sklearn.linear_model.LogisticRegression. Which has the higher average (over all 4 classes) F1 (no hand-in)?"],"metadata":{"id":"wMCuEriHXLZE"}},{"cell_type":"code","source":["log_reg.fit(X_train, y_train)\n","yhat = log_reg.predict(X_test)\n","print(classification_report(y_test, yhat, digits=3))"],"metadata":{"id":"JEQ3X7faYTky"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hz4yA62HAyB6"},"source":["# TASK 2. Cross Validation (5 points)\n","\n","Using the same toy dataset as in Task 1, use cross-validation instead of a fixed train-test data split. In particular, implement a function ``kFoldCV`` that takes as parameters the number of examples n in a dataset, and the number of folds  k. The function should return two arrays of indices -- idxsTr and idxsTe -- which specify the indices of the training and testing examples (respectively) in the k different folds. In particular, idxsTe is an array with k elements, whereby idxsTe[0] should contain the first n/k indices; idxsTe[1] should contain the next n/k examples; and so on. Correspondingly, idxsTr is an array with k elements, whereby idxsTr[i] contains all the indices [0,1,2,...,n-1] *except* those in idxsTe[i].\n","If n is not exactly divisible by k, then only the *last* testing fold (i.e., idxsTe[k-1]) should receive fewer examples.\n","\n","Example 1: ``kFoldCV(n=6, k=3)`` should return:\n","```\n","idxsTe = [ [ 0, 1 ], [ 2, 3 ], [ 4, 5 ] ]\n","idxsTr = [ [ 2, 3, 4, 5 ], [ 0, 1, 4, 5 ], [ 0, 1, 2, 3 ] ]\n","```\n","\n","Example 2: ``kFoldCV(n=5, k=3)`` should return:\n","```\n","idxsTe = [ [ 0, 1 ], [ 2, 3 ], [ 4 ] ]\n","idxsTr = [ [ 2, 3, 4 ], [ 0, 1, 4 ], [ 0, 1, 2, 3 ] ]\n","```"]},{"cell_type":"code","source":["def kFoldCV (n: int, k: int):\n","  idxsTr = []\n","  idxsTe = []\n","  # TODO: implement me\n","  return idxsTr, idxsTe"],"metadata":{"id":"CHydmKAX1U2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kFoldCV(n=5, k=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQKdCsf3-Qm5","executionInfo":{"status":"ok","timestamp":1697709776700,"user_tz":-120,"elapsed":2,"user":{"displayName":"Jacob Whitehill","userId":"14092880161776374995"}},"outputId":"eceaaa2a-62c8-40c5-d343-1a96c6ea80ca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([array([2, 3, 4]), array([0, 1, 4]), array([0, 1, 2, 3])],\n"," [array([0, 1]), array([2, 3]), array([4])])"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["Now, using the kFoldCV method you just implemented, compute the cross-validation accuracy on the synthetic dataset from Task 1 using k=5 folds: For each fold, train on the training portion and then test on the testing portion. Report the mean, as well as the standard deviation, of accuracy (i.e., proportion of examples that are correctly classified) over all 5 folds."],"metadata":{"id":"nc5a7CJX_Aal"}},{"cell_type":"code","source":["def computeCVAccuracy (k):\n","  return 0  # TODO: implement me"],"metadata":{"id":"j4IiF-Yd_Nmk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, vary the number of folds k over the set [ 5, 10, 15, ..., 30 ], and recompute the mean cross-validation accuracy and its standard deviation each time (for each k). What trend do you observe? Why do you think it happens? Based on your result, name one advantage and one disadvantage of performing k-fold cross-validation with a large k value."],"metadata":{"id":"1rHPUteJA-Rc"}},{"cell_type":"code","source":["# TODO: implement me"],"metadata":{"id":"RTKqD0ybBPOj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y3w3Hop_46Af"},"source":["## ðŸ“¢ **HAND-IN** ðŸ“¢: Answer following question in Moodle:\n","\n","On Moodle, submit the accuracy scores and their standard deviations for the increasing values of k, and explain the trend."]},{"cell_type":"markdown","metadata":{"id":"TBJKSAuw79su"},"source":["# TASK 3. Optimal Threshold Selection (2 points)\n","Probabilistic classifiers such as logistic regression output a real number between 0 and 1, which expresses how certain/confident the model is that the input example belongs to the positive class. However, to make a final (binary) classification decision, we have to apply some threshold to this number. Oftentimes, 0.5 is taken as the threshold, which is intuitive since it is halfway between 0 and 1. However, this value is actually arbitrary.\n","\n","A more sensible way of picking the threshold is based on the **cost** involved in making different kinds of mistakes. In binary decision problems, there can be false positives (mistakenly classifying a negative example as a positive) and false negatives (mistakenly classifying a positive example as a negative). These two kinds of mistakes may have different real-world costs associated with them. For instance, in a machine learning setting in which a credit card company wants to predict whether a customer will enter \"default\"/bankruptcy and wants to automatically \"warn\" the customer not to forget their payment, the costs could be:\n","- False positive: Telling a customer you are concerned they might not pay their bill, even though they always do, can be insulting. It could cause them to move their business elsewhere.\n","- False negative: Not telling a customer you are concerned even though they will forget to pay a bill can result in lost revenue for the company.\n","\n","Based on the relative magnitude of these costs, it may be more important to keep the number of false positives low at the expense of accruing more false negatives; or the reverse might be true.\n","\n","With this in mind, this task will explore how to calculate an optimal threshold automatically, given a pre-defined cost configuration of false positives and false negatives.\n","\n","### Dataset:\n","In this task we will work with the **Default of Credit Card Clients Dataset**. This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. More info about the dataset can be found [here](https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset).\n","\n","The target variable is:\n","\n","- `default.payment.next.month`: Default payment (1=yes, 0=no)\n","\n","Default payment means a missed payment. So, the target variable shows whether a person will miss his or her Credit Card Payment (=1) or will pay it back (=0).\n","\n","The predictor features are:\n","*   ID: ID of each client\n","*   LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n","*   SEX:0, SEX:1: Binary variables for gender (0=male, 1=female)\n","*   EDUCATION:0-EDUCATION:5: Binary variables for education (0=graduate school, 1=university, 2=high school, 3=others, 4=unknown, 5=unknown)\n","*   MARRIAGE:0-MARRIAGE:2: Binary variables for Marital status (0=married, 1=single, 2=others)\n","*   AGE: Age in years\n","*   PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, â€¦ 8=payment delay for eight months, 9=payment delay for nine months and above)\n","*   PAY_2: Repayment status in August, 2005 (scale same as above)\n","*   PAY_3: Repayment status in July, 2005 (scale same as above)\n","*   PAY_4: Repayment status in June, 2005 (scale same as above)\n","*   PAY_5: Repayment status in May, 2005 (scale same as above)\n","*   PAY_6: Repayment status in April, 2005 (scale same as above)\n","*   BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n","*   BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n","*   BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n","*   BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n","*   BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n","*   BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n","*   PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n","*   PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n","*   PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n","*   PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n","*   PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n","*   PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)"]},{"cell_type":"code","source":["# install datasets\n","!pip install datasets"],"metadata":{"id":"9ULG3coNPvLX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First, we load and inspect the data."],"metadata":{"id":"b1qRR3NsPzdj"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"imodels/credit-card\")\n","df_train = pd.DataFrame(dataset['train'])\n","X_train = df_train.drop(columns=['default.payment.next.month'])\n","y_train = df_train['default.payment.next.month'].values\n","print(X_train.head())\n","\n","df_test = pd.DataFrame(dataset['test'])\n","X_test = df_test.drop(columns=['default.payment.next.month'])\n","y_test = df_test['default.payment.next.month'].values"],"metadata":{"id":"MFCSTCd5P2cf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###  Standardization of the data\n","We first apply \"Standardization\" to the data, i.e. we scale the data such that they look more or less like standard normally distributed. You can use the `StandardScaler` from the `sklearn` library, which subtracts the mean and divides by the standard deviation of each feature (as computed on the training set). This can sometimes improve prediction accuracy."],"metadata":{"id":"fGaoOKZDP_fB"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","scaler.fit(X_train)\n","\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"qKuvQhmyQKIE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Logistic Regression\n","We now apply Logistic regression to the data in order to predict `default.payment.next.month`."],"metadata":{"id":"qFMfir2LQRqW"}},{"cell_type":"code","source":["log_reg = LogisticRegression()\n","log_reg.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"r9JEwcWRQatR","executionInfo":{"status":"ok","timestamp":1697709869274,"user_tz":-120,"elapsed":628,"user":{"displayName":"Jacob Whitehill","userId":"14092880161776374995"}},"outputId":"6ebcc662-a3ba-4a21-81ec-d05714709337"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression()"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["### TASK: Threshold Optimization\n","Given a false positive (i.e., incorrectly predict that someone will default) cost of 100 NT, and a false negative (i.e., incorrectly predict that someone will not default) cost of 1000 NT, and given a set of possible threshold values [ 0.1, 0.2, ..., 0.9 ], what is the best threshold?"],"metadata":{"id":"2oVufma4RBSD"}},{"cell_type":"code","source":["def findBestThreshold (fpCost, fnCost):\n","  return 0  # TODO: implement me"],"metadata":{"id":"88MmiK4vRU9-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Suppose the costs of a false negative and a false positive are more similar, e.g., both are 500 NT. What is the best threshold then?"],"metadata":{"id":"hh4cJhtsSHY2"}},{"cell_type":"markdown","metadata":{"id":"-JkHrkdl97W-"},"source":["## ðŸ“¢ **HAND-IN** ðŸ“¢: Please hand in the following on Moodle:\n","\n","1. Your implementation of findBestThreshold.\n","2. What is the best threshold for the costs of 100 NT dollars for false positives and 1000 NT dollars for false negatives?\n","3. What is the best threshold for the costs of 500 NT dollars for false positives and 500 NT dollars for false negatives?"]},{"cell_type":"code","source":[],"metadata":{"id":"dYEpDz2kTV0P"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1wEBkfXpblsxIsCP-Pl7ucA7ecj2D96oz","timestamp":1697633014103},{"file_id":"1a8JYfGm1gMPT5yWhnTgQxMueaUIYsmJp","timestamp":1665570545773},{"file_id":"1pvn6jufgTJDs05Wbx9AL1OWsz8xcf6PR","timestamp":1664870982154},{"file_id":"12sZ0VrTiGnJUGujCtivupha9XUc5_R4O","timestamp":1664796966493}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}